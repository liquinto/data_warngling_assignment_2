{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96765a09-de2c-47d4-9a68-9ea4b2a0b6f4",
   "metadata": {},
   "source": [
    "# INFOMDWR â€“ Assignment 2: Data Integration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce808c57-f79c-4f3b-ac74-d10da39d4e8f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment, you will work on different tasks that are related to data preparation including data profiling, finding records that refer to same entity, and computing the correlation between different attributes. The datasets that you need to work on are available online (links are provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6092e9-4843-4b76-95f0-e7f73e2710e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any\n",
    "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
    "from py_stringmatching.similarity_measure.jaro import Jaro\n",
    "from py_stringmatching.similarity_measure.affine import Affine\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a98575-577b-4329-965e-5e4b78216bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collision_index</th>\n",
       "      <th>collision_year</th>\n",
       "      <th>collision_ref_no</th>\n",
       "      <th>location_easting_osgr</th>\n",
       "      <th>location_northing_osgr</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>police_force</th>\n",
       "      <th>collision_severity</th>\n",
       "      <th>number_of_vehicles</th>\n",
       "      <th>...</th>\n",
       "      <th>carriageway_hazards_historic</th>\n",
       "      <th>carriageway_hazards</th>\n",
       "      <th>urban_or_rural_area</th>\n",
       "      <th>did_police_officer_attend_scene_of_accident</th>\n",
       "      <th>trunk_road_flag</th>\n",
       "      <th>lsoa_of_accident_location</th>\n",
       "      <th>enhanced_severity_collision</th>\n",
       "      <th>collision_injury_based</th>\n",
       "      <th>collision_adjusted_severity_serious</th>\n",
       "      <th>collision_adjusted_severity_slight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202417H103224</td>\n",
       "      <td>2024</td>\n",
       "      <td>17H103224</td>\n",
       "      <td>448894</td>\n",
       "      <td>532505</td>\n",
       "      <td>-1.24312</td>\n",
       "      <td>54.68523</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>E01011983</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202417M217924</td>\n",
       "      <td>2024</td>\n",
       "      <td>17M217924</td>\n",
       "      <td>452135</td>\n",
       "      <td>519436</td>\n",
       "      <td>-1.19517</td>\n",
       "      <td>54.56747</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>E01012061</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202417S204524</td>\n",
       "      <td>2024</td>\n",
       "      <td>17S204524</td>\n",
       "      <td>445427</td>\n",
       "      <td>522924</td>\n",
       "      <td>-1.29837</td>\n",
       "      <td>54.59946</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01012280</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111621</td>\n",
       "      <td>0.888379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024481510889</td>\n",
       "      <td>2024</td>\n",
       "      <td>481510889</td>\n",
       "      <td>533587</td>\n",
       "      <td>181174</td>\n",
       "      <td>-0.07626</td>\n",
       "      <td>51.51371</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01000005</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024481563500</td>\n",
       "      <td>2024</td>\n",
       "      <td>481563500</td>\n",
       "      <td>532676</td>\n",
       "      <td>180902</td>\n",
       "      <td>-0.08948</td>\n",
       "      <td>51.51148</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01032739</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>202417S114624</td>\n",
       "      <td>2024</td>\n",
       "      <td>17S114624</td>\n",
       "      <td>443610</td>\n",
       "      <td>520960</td>\n",
       "      <td>-1.32678</td>\n",
       "      <td>54.58197</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01012271</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024481561505</td>\n",
       "      <td>2024</td>\n",
       "      <td>481561505</td>\n",
       "      <td>531722</td>\n",
       "      <td>181452</td>\n",
       "      <td>-0.10302</td>\n",
       "      <td>51.51664</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01032740</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202417S203724</td>\n",
       "      <td>2024</td>\n",
       "      <td>17S203724</td>\n",
       "      <td>446255</td>\n",
       "      <td>525635</td>\n",
       "      <td>-1.28513</td>\n",
       "      <td>54.62375</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01012243</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127317</td>\n",
       "      <td>0.872683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202417S303424</td>\n",
       "      <td>2024</td>\n",
       "      <td>17S303424</td>\n",
       "      <td>443934</td>\n",
       "      <td>514917</td>\n",
       "      <td>-1.32267</td>\n",
       "      <td>54.52764</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>E01033475</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170336</td>\n",
       "      <td>0.829664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202417M109924</td>\n",
       "      <td>2024</td>\n",
       "      <td>17M109924</td>\n",
       "      <td>449627</td>\n",
       "      <td>517955</td>\n",
       "      <td>-1.23421</td>\n",
       "      <td>54.55441</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>E01033469</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  collision_index  collision_year collision_ref_no  location_easting_osgr  \\\n",
       "0   202417H103224            2024        17H103224                 448894   \n",
       "1   202417M217924            2024        17M217924                 452135   \n",
       "2   202417S204524            2024        17S204524                 445427   \n",
       "3   2024481510889            2024        481510889                 533587   \n",
       "4   2024481563500            2024        481563500                 532676   \n",
       "5   202417S114624            2024        17S114624                 443610   \n",
       "6   2024481561505            2024        481561505                 531722   \n",
       "7   202417S203724            2024        17S203724                 446255   \n",
       "8   202417S303424            2024        17S303424                 443934   \n",
       "9   202417M109924            2024        17M109924                 449627   \n",
       "\n",
       "   location_northing_osgr  longitude  latitude  police_force  \\\n",
       "0                  532505   -1.24312  54.68523            17   \n",
       "1                  519436   -1.19517  54.56747            17   \n",
       "2                  522924   -1.29837  54.59946            17   \n",
       "3                  181174   -0.07626  51.51371            48   \n",
       "4                  180902   -0.08948  51.51148            48   \n",
       "5                  520960   -1.32678  54.58197            17   \n",
       "6                  181452   -0.10302  51.51664            48   \n",
       "7                  525635   -1.28513  54.62375            17   \n",
       "8                  514917   -1.32267  54.52764            17   \n",
       "9                  517955   -1.23421  54.55441            17   \n",
       "\n",
       "   collision_severity  number_of_vehicles  ...  carriageway_hazards_historic  \\\n",
       "0                   3                   2  ...                            -1   \n",
       "1                   2                   2  ...                            -1   \n",
       "2                   3                   2  ...                             0   \n",
       "3                   2                   1  ...                            -1   \n",
       "4                   2                   1  ...                            -1   \n",
       "5                   2                   1  ...                            -1   \n",
       "6                   3                   2  ...                            -1   \n",
       "7                   3                   2  ...                             0   \n",
       "8                   3                   1  ...                             0   \n",
       "9                   3                   2  ...                            -1   \n",
       "\n",
       "  carriageway_hazards  urban_or_rural_area  \\\n",
       "0                   0                    1   \n",
       "1                   0                    1   \n",
       "2                   0                    2   \n",
       "3                   0                    1   \n",
       "4                   0                    1   \n",
       "5                   0                    1   \n",
       "6                   0                    1   \n",
       "7                   0                    1   \n",
       "8                   0                    2   \n",
       "9                   0                    1   \n",
       "\n",
       "  did_police_officer_attend_scene_of_accident  trunk_road_flag  \\\n",
       "0                                           2                2   \n",
       "1                                           3                2   \n",
       "2                                           1                2   \n",
       "3                                           1                2   \n",
       "4                                           1                2   \n",
       "5                                           1                2   \n",
       "6                                           1                2   \n",
       "7                                           1                2   \n",
       "8                                           1                2   \n",
       "9                                           3                2   \n",
       "\n",
       "  lsoa_of_accident_location enhanced_severity_collision  \\\n",
       "0                 E01011983                           3   \n",
       "1                 E01012061                           7   \n",
       "2                 E01012280                          -1   \n",
       "3                 E01000005                           7   \n",
       "4                 E01032739                           5   \n",
       "5                 E01012271                           7   \n",
       "6                 E01032740                           3   \n",
       "7                 E01012243                          -1   \n",
       "8                 E01033475                          -1   \n",
       "9                 E01033469                           3   \n",
       "\n",
       "  collision_injury_based  collision_adjusted_severity_serious  \\\n",
       "0                      1                             0.000000   \n",
       "1                      1                             1.000000   \n",
       "2                      0                             0.111621   \n",
       "3                      1                             1.000000   \n",
       "4                      1                             1.000000   \n",
       "5                      1                             1.000000   \n",
       "6                      1                             0.000000   \n",
       "7                      0                             0.127317   \n",
       "8                      0                             0.170336   \n",
       "9                      1                             0.000000   \n",
       "\n",
       "   collision_adjusted_severity_slight  \n",
       "0                            1.000000  \n",
       "1                            0.000000  \n",
       "2                            0.888379  \n",
       "3                            0.000000  \n",
       "4                            0.000000  \n",
       "5                            0.000000  \n",
       "6                            1.000000  \n",
       "7                            0.872683  \n",
       "8                            0.829664  \n",
       "9                            1.000000  \n",
       "\n",
       "[10 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv into a dataframe and print a part of the dataframe\n",
    "\n",
    "df = pd.read_csv(\"dft-road-casualty-statistics-collision-2024.csv\", low_memory = False)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed268c-64bc-40ef-9635-2943d5093ce0",
   "metadata": {},
   "source": [
    "## Task 1: Profiling relational data\n",
    "\n",
    "For this task, download and read the paper about profiling relational data, select a set of summary statistics about the data (minimum of 10 different values) and write Python code to compute these quantities for a dataset of your choice. Preferably, you can use one of the csv files from the road safety dataset. Explain the importance of each summary statistic that you selected in understanding the characteristics of the dataset.\n",
    "\n",
    "*Note: Computing the same statistical quantity on multiple columns of the dataset will be counted only once.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee272e1f-1357-4e92-bb6e-5733744fcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to hold the summary statistics\n",
    "\n",
    "summary_statistics: dict[str, Any] = {\n",
    "    \"mean\": None,\n",
    "    \"median\": None,\n",
    "    \"mode\": None,\n",
    "    \"minimum\": None,\n",
    "    \"maximum\": None,\n",
    "    \"range\": None,\n",
    "    \"variance\": None,\n",
    "    \"standard_deviation\": None,\n",
    "    \"records\": None,\n",
    "    \"number_of_missing_records\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed94994-70cd-4538-aac1-6dac87a690a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 35.87966550080751,\n",
       " 'median': 30.0,\n",
       " 'mode': 0    30\n",
       " Name: speed_limit, dtype: int64,\n",
       " 'minimum': -1,\n",
       " 'maximum': 70,\n",
       " 'range': 71,\n",
       " 'variance': 211.30421831349014,\n",
       " 'standard_deviation': 14.536306900774012,\n",
       " 'records': 100927,\n",
       " 'number_of_missing_records': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill the dictionary for summary statistics\n",
    "\n",
    "summary_statistics[\"mean\"] = df[\"speed_limit\"].mean()                                           # Mean of the speed limit\n",
    "summary_statistics[\"median\"] = df[\"speed_limit\"].median()                                       # Median of the speed limit \n",
    "summary_statistics[\"mode\"] = df[\"speed_limit\"].mode()                                           # Mode of the speed limit\n",
    "summary_statistics[\"minimum\"] = df[\"speed_limit\"].min()                                         # Min of the speed limit\n",
    "summary_statistics[\"maximum\"] = df[\"speed_limit\"].max()                                         # Max of the speed limit\n",
    "summary_statistics[\"range\"] = abs(df[\"speed_limit\"].max()) + abs(df[\"speed_limit\"].min())       # Range of the speed limit, thus abs(max)+ and(min))\n",
    "summary_statistics[\"variance\"] = df[\"speed_limit\"].var()                                        # Variance of the speed limit\n",
    "summary_statistics[\"standard_deviation\"] = df[\"speed_limit\"].std()                              # Standard deviation of the speed limit\n",
    "summary_statistics[\"records\"] = len(df[\"speed_limit\"])                                          # Number of rows in the dataframe\n",
    "summary_statistics[\"number_of_missing_records\"] = df.isnull().sum().sum()                       # Number of missing values in the entire dataframe\n",
    "\n",
    "summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b98bbe-e4db-410e-ada7-ce9bfbc2a5c4",
   "metadata": {},
   "source": [
    "## Task 2: Entity resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd598718-2a4c-4cd5-9cf1-291d8ded1bfb",
   "metadata": {},
   "source": [
    "### Part 1:\n",
    "\n",
    "Write a Python code to compare every single record in the dataset (ACM.csv) with all the records in (DBLP2.csv) and find the similar records (records that represent the same publication). To compare two records, follow the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8a3386-2363-4be4-a598-bd1d8808210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "\n",
    "df_acm = pd.read_csv(\"ACM.csv\")\n",
    "df_dblp2 = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d176b90-d11b-40a4-ba35-15c9a6fb5159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304586</td>\n",
       "      <td>The WASA2 object-oriented workflow management ...</td>\n",
       "      <td>Gottfried Vossen, Mathias Weske</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304587</td>\n",
       "      <td>A user-centered interface for querying distrib...</td>\n",
       "      <td>Isabel F. Cruz, Kimberly M. James</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304589</td>\n",
       "      <td>World Wide Database-integrating the Web, CORBA...</td>\n",
       "      <td>Athman Bouguettaya, Boualem Benatallah, Lily H...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304590</td>\n",
       "      <td>XML-based information mediation with MIX</td>\n",
       "      <td>Chaitan Baru, Amarnath Gupta, Bertram Lud&amp;#228...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304582</td>\n",
       "      <td>The CCUBE constraint object-oriented database ...</td>\n",
       "      <td>Alexander Brodsky, Victor E. Segal, Jia Chen, ...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>304583</td>\n",
       "      <td>The Cornell Jaguar project: adding mobility to...</td>\n",
       "      <td>Phillippe Bonnet, Kyle Buza, Zhiyuan Chan, Vic...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>304584</td>\n",
       "      <td>The active MultiSync controller of the cubetre...</td>\n",
       "      <td>Nick Roussopoulos, Yannis Kotidis, Yannis Sism...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>304585</td>\n",
       "      <td>The Jungle database search engine</td>\n",
       "      <td>Michael B&amp;#246;hlen, Linas Bukauskas, Curtis D...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>306112</td>\n",
       "      <td>ADEPT: an agent-based approach to business pro...</td>\n",
       "      <td>N. R. Jennings, T. J. Norman, P. Faratin</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>306115</td>\n",
       "      <td>A componentized architecture for dynamic elect...</td>\n",
       "      <td>Benny Reich, Israel Ben-Shaul</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  304586  The WASA2 object-oriented workflow management ...   \n",
       "1  304587  A user-centered interface for querying distrib...   \n",
       "2  304589  World Wide Database-integrating the Web, CORBA...   \n",
       "3  304590           XML-based information mediation with MIX   \n",
       "4  304582  The CCUBE constraint object-oriented database ...   \n",
       "5  304583  The Cornell Jaguar project: adding mobility to...   \n",
       "6  304584  The active MultiSync controller of the cubetre...   \n",
       "7  304585                  The Jungle database search engine   \n",
       "8  306112  ADEPT: an agent-based approach to business pro...   \n",
       "9  306115  A componentized architecture for dynamic elect...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                    Gottfried Vossen, Mathias Weske   \n",
       "1                  Isabel F. Cruz, Kimberly M. James   \n",
       "2  Athman Bouguettaya, Boualem Benatallah, Lily H...   \n",
       "3  Chaitan Baru, Amarnath Gupta, Bertram Lud&#228...   \n",
       "4  Alexander Brodsky, Victor E. Segal, Jia Chen, ...   \n",
       "5  Phillippe Bonnet, Kyle Buza, Zhiyuan Chan, Vic...   \n",
       "6  Nick Roussopoulos, Yannis Kotidis, Yannis Sism...   \n",
       "7  Michael B&#246;hlen, Linas Bukauskas, Curtis D...   \n",
       "8           N. R. Jennings, T. J. Norman, P. Faratin   \n",
       "9                      Benny Reich, Israel Ben-Shaul   \n",
       "\n",
       "                                            venue  year  \n",
       "0  International Conference on Management of Data  1999  \n",
       "1  International Conference on Management of Data  1999  \n",
       "2  International Conference on Management of Data  1999  \n",
       "3  International Conference on Management of Data  1999  \n",
       "4  International Conference on Management of Data  1999  \n",
       "5  International Conference on Management of Data  1999  \n",
       "6  International Conference on Management of Data  1999  \n",
       "7  International Conference on Management of Data  1999  \n",
       "8                              ACM SIGMOD Record   1998  \n",
       "9                              ACM SIGMOD Record   1998  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the dataframe of ACM\n",
    "\n",
    "df_acm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d7edf1-6712-417f-b524-8a2134c2eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journals/sigmod/Mackay99</td>\n",
       "      <td>Semantic Integration of Environmental Models f...</td>\n",
       "      <td>D. Scott Mackay</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf/vldb/PoosalaI96</td>\n",
       "      <td>Estimation of Query-Result Distribution and it...</td>\n",
       "      <td>Viswanath Poosala, Yannis E. Ioannidis</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/vldb/PalpanasSCP02</td>\n",
       "      <td>Incremental Maintenance for Non-Distributive A...</td>\n",
       "      <td>Themistoklis Palpanas, Richard Sidle, Hamid Pi...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf/vldb/GardarinGT96</td>\n",
       "      <td>Cost-based Selection of Path Expression Proces...</td>\n",
       "      <td>Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf/vldb/HoelS95</td>\n",
       "      <td>Benchmarking Spatial Join Operations with Spat...</td>\n",
       "      <td>Erik G. Hoel, Hanan Samet</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conf/sigmod/Keim99</td>\n",
       "      <td>Efficient Geometry-based Similarity Search of ...</td>\n",
       "      <td>Daniel A. Keim</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>journals/sigmod/Ouksel02</td>\n",
       "      <td>Mining the World Wide Web: An Information Sear...</td>\n",
       "      <td>Aris M. Ouksel</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>journals/vldb/Seshadri98</td>\n",
       "      <td>Enhanced Abstract Data Types in Object-Relatio...</td>\n",
       "      <td>Praveen Seshadri</td>\n",
       "      <td>VLDB J.</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>journals/sigmod/RamamrithamS97</td>\n",
       "      <td>Report on DART '96: Databases: Active and Real...</td>\n",
       "      <td>Nandit Soparkar, Krithi Ramamritham</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>journals/sigmod/DAndreaJ96</td>\n",
       "      <td>UniSQL's Next-Generation Object-Relational Dat...</td>\n",
       "      <td>Phil Janus, Albert D'Andrea</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0        journals/sigmod/Mackay99   \n",
       "1            conf/vldb/PoosalaI96   \n",
       "2         conf/vldb/PalpanasSCP02   \n",
       "3          conf/vldb/GardarinGT96   \n",
       "4               conf/vldb/HoelS95   \n",
       "5              conf/sigmod/Keim99   \n",
       "6        journals/sigmod/Ouksel02   \n",
       "7        journals/vldb/Seshadri98   \n",
       "8  journals/sigmod/RamamrithamS97   \n",
       "9      journals/sigmod/DAndreaJ96   \n",
       "\n",
       "                                               title  \\\n",
       "0  Semantic Integration of Environmental Models f...   \n",
       "1  Estimation of Query-Result Distribution and it...   \n",
       "2  Incremental Maintenance for Non-Distributive A...   \n",
       "3  Cost-based Selection of Path Expression Proces...   \n",
       "4  Benchmarking Spatial Join Operations with Spat...   \n",
       "5  Efficient Geometry-based Similarity Search of ...   \n",
       "6  Mining the World Wide Web: An Information Sear...   \n",
       "7  Enhanced Abstract Data Types in Object-Relatio...   \n",
       "8  Report on DART '96: Databases: Active and Real...   \n",
       "9  UniSQL's Next-Generation Object-Relational Dat...   \n",
       "\n",
       "                                             authors              venue  year  \n",
       "0                                    D. Scott Mackay      SIGMOD Record  1999  \n",
       "1             Viswanath Poosala, Yannis E. Ioannidis               VLDB  1996  \n",
       "2  Themistoklis Palpanas, Richard Sidle, Hamid Pi...               VLDB  2002  \n",
       "3  Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...               VLDB  1996  \n",
       "4                          Erik G. Hoel, Hanan Samet               VLDB  1995  \n",
       "5                                     Daniel A. Keim  SIGMOD Conference  1999  \n",
       "6                                     Aris M. Ouksel      SIGMOD Record  2002  \n",
       "7                                   Praveen Seshadri            VLDB J.  1998  \n",
       "8                Nandit Soparkar, Krithi Ramamritham      SIGMOD Record  1997  \n",
       "9                        Phil Janus, Albert D'Andrea      SIGMOD Record  1996  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the dataframe of dblp2\n",
    "\n",
    "df_dblp2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31341a-5a97-468e-a3e6-9fdadaaa56c5",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "### A. \n",
    "\n",
    "Ignore the pub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d281a4-2cce-4967-97e2-5b4c3fca9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acm.drop(\"id\", axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d3ff88-4d31-433a-80e0-29c88a5be158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp2.drop(\"id\", axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c0854-19a6-4467-a694-da679894ab20",
   "metadata": {},
   "source": [
    "### B. \n",
    "\n",
    "Change all alphabetical characters into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5a4c03-6e1d-4c56-b5bd-d9961bc54c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_acm.columns:\n",
    "    df_acm[col] = df_acm[col].astype(str).str.lower()\n",
    "for col in df_dblp2.columns:\n",
    "    df_dblp2[col] = df_dblp2[col].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9478fd61-8679-43e0-aa9d-d99b04982f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the wasa2 object-oriented workflow management ...</td>\n",
       "      <td>gottfried vossen, mathias weske</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a user-centered interface for querying distrib...</td>\n",
       "      <td>isabel f. cruz, kimberly m. james</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world wide database-integrating the web, corba...</td>\n",
       "      <td>athman bouguettaya, boualem benatallah, lily h...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xml-based information mediation with mix</td>\n",
       "      <td>chaitan baru, amarnath gupta, bertram lud&amp;#228...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ccube constraint object-oriented database ...</td>\n",
       "      <td>alexander brodsky, victor e. segal, jia chen, ...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the cornell jaguar project: adding mobility to...</td>\n",
       "      <td>phillippe bonnet, kyle buza, zhiyuan chan, vic...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the active multisync controller of the cubetre...</td>\n",
       "      <td>nick roussopoulos, yannis kotidis, yannis sism...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the jungle database search engine</td>\n",
       "      <td>michael b&amp;#246;hlen, linas bukauskas, curtis d...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adept: an agent-based approach to business pro...</td>\n",
       "      <td>n. r. jennings, t. j. norman, p. faratin</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a componentized architecture for dynamic elect...</td>\n",
       "      <td>benny reich, israel ben-shaul</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  the wasa2 object-oriented workflow management ...   \n",
       "1  a user-centered interface for querying distrib...   \n",
       "2  world wide database-integrating the web, corba...   \n",
       "3           xml-based information mediation with mix   \n",
       "4  the ccube constraint object-oriented database ...   \n",
       "5  the cornell jaguar project: adding mobility to...   \n",
       "6  the active multisync controller of the cubetre...   \n",
       "7                  the jungle database search engine   \n",
       "8  adept: an agent-based approach to business pro...   \n",
       "9  a componentized architecture for dynamic elect...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                    gottfried vossen, mathias weske   \n",
       "1                  isabel f. cruz, kimberly m. james   \n",
       "2  athman bouguettaya, boualem benatallah, lily h...   \n",
       "3  chaitan baru, amarnath gupta, bertram lud&#228...   \n",
       "4  alexander brodsky, victor e. segal, jia chen, ...   \n",
       "5  phillippe bonnet, kyle buza, zhiyuan chan, vic...   \n",
       "6  nick roussopoulos, yannis kotidis, yannis sism...   \n",
       "7  michael b&#246;hlen, linas bukauskas, curtis d...   \n",
       "8           n. r. jennings, t. j. norman, p. faratin   \n",
       "9                      benny reich, israel ben-shaul   \n",
       "\n",
       "                                            venue  year  \n",
       "0  international conference on management of data  1999  \n",
       "1  international conference on management of data  1999  \n",
       "2  international conference on management of data  1999  \n",
       "3  international conference on management of data  1999  \n",
       "4  international conference on management of data  1999  \n",
       "5  international conference on management of data  1999  \n",
       "6  international conference on management of data  1999  \n",
       "7  international conference on management of data  1999  \n",
       "8                              acm sigmod record   1998  \n",
       "9                              acm sigmod record   1998  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "573343df-2c41-49f2-8335-8f244f0d24d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic integration of environmental models f...</td>\n",
       "      <td>d. scott mackay</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estimation of query-result distribution and it...</td>\n",
       "      <td>viswanath poosala, yannis e. ioannidis</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incremental maintenance for non-distributive a...</td>\n",
       "      <td>themistoklis palpanas, richard sidle, hamid pi...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cost-based selection of path expression proces...</td>\n",
       "      <td>zhao-hui tang, georges gardarin, jean-robert g...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benchmarking spatial join operations with spat...</td>\n",
       "      <td>erik g. hoel, hanan samet</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>efficient geometry-based similarity search of ...</td>\n",
       "      <td>daniel a. keim</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mining the world wide web: an information sear...</td>\n",
       "      <td>aris m. ouksel</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enhanced abstract data types in object-relatio...</td>\n",
       "      <td>praveen seshadri</td>\n",
       "      <td>vldb j.</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>report on dart '96: databases: active and real...</td>\n",
       "      <td>nandit soparkar, krithi ramamritham</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unisql's next-generation object-relational dat...</td>\n",
       "      <td>phil janus, albert d'andrea</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  semantic integration of environmental models f...   \n",
       "1  estimation of query-result distribution and it...   \n",
       "2  incremental maintenance for non-distributive a...   \n",
       "3  cost-based selection of path expression proces...   \n",
       "4  benchmarking spatial join operations with spat...   \n",
       "5  efficient geometry-based similarity search of ...   \n",
       "6  mining the world wide web: an information sear...   \n",
       "7  enhanced abstract data types in object-relatio...   \n",
       "8  report on dart '96: databases: active and real...   \n",
       "9  unisql's next-generation object-relational dat...   \n",
       "\n",
       "                                             authors              venue  year  \n",
       "0                                    d. scott mackay      sigmod record  1999  \n",
       "1             viswanath poosala, yannis e. ioannidis               vldb  1996  \n",
       "2  themistoklis palpanas, richard sidle, hamid pi...               vldb  2002  \n",
       "3  zhao-hui tang, georges gardarin, jean-robert g...               vldb  1996  \n",
       "4                          erik g. hoel, hanan samet               vldb  1995  \n",
       "5                                     daniel a. keim  sigmod conference  1999  \n",
       "6                                     aris m. ouksel      sigmod record  2002  \n",
       "7                                   praveen seshadri            vldb j.  1998  \n",
       "8                nandit soparkar, krithi ramamritham      sigmod record  1997  \n",
       "9                        phil janus, albert d'andrea      sigmod record  1996  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dblp2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b787fa0-9b6d-482c-9396-2148f2703841",
   "metadata": {},
   "source": [
    "### C. \n",
    "\n",
    "Convert multiple spaces to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cf5f35-56d4-492b-8704-21418973cd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the wasa2 object-oriented workflow management ...</td>\n",
       "      <td>gottfried vossen, mathias weske</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a user-centered interface for querying distrib...</td>\n",
       "      <td>isabel f. cruz, kimberly m. james</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world wide database-integrating the web, corba...</td>\n",
       "      <td>athman bouguettaya, boualem benatallah, lily h...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xml-based information mediation with mix</td>\n",
       "      <td>chaitan baru, amarnath gupta, bertram lud&amp;#228...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ccube constraint object-oriented database ...</td>\n",
       "      <td>alexander brodsky, victor e. segal, jia chen, ...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the cornell jaguar project: adding mobility to...</td>\n",
       "      <td>phillippe bonnet, kyle buza, zhiyuan chan, vic...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the active multisync controller of the cubetre...</td>\n",
       "      <td>nick roussopoulos, yannis kotidis, yannis sism...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the jungle database search engine</td>\n",
       "      <td>michael b&amp;#246;hlen, linas bukauskas, curtis d...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adept: an agent-based approach to business pro...</td>\n",
       "      <td>n. r. jennings, t. j. norman, p. faratin</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a componentized architecture for dynamic elect...</td>\n",
       "      <td>benny reich, israel ben-shaul</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  the wasa2 object-oriented workflow management ...   \n",
       "1  a user-centered interface for querying distrib...   \n",
       "2  world wide database-integrating the web, corba...   \n",
       "3           xml-based information mediation with mix   \n",
       "4  the ccube constraint object-oriented database ...   \n",
       "5  the cornell jaguar project: adding mobility to...   \n",
       "6  the active multisync controller of the cubetre...   \n",
       "7                  the jungle database search engine   \n",
       "8  adept: an agent-based approach to business pro...   \n",
       "9  a componentized architecture for dynamic elect...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                    gottfried vossen, mathias weske   \n",
       "1                  isabel f. cruz, kimberly m. james   \n",
       "2  athman bouguettaya, boualem benatallah, lily h...   \n",
       "3  chaitan baru, amarnath gupta, bertram lud&#228...   \n",
       "4  alexander brodsky, victor e. segal, jia chen, ...   \n",
       "5  phillippe bonnet, kyle buza, zhiyuan chan, vic...   \n",
       "6  nick roussopoulos, yannis kotidis, yannis sism...   \n",
       "7  michael b&#246;hlen, linas bukauskas, curtis d...   \n",
       "8           n. r. jennings, t. j. norman, p. faratin   \n",
       "9                      benny reich, israel ben-shaul   \n",
       "\n",
       "                                            venue  year  \n",
       "0  international conference on management of data  1999  \n",
       "1  international conference on management of data  1999  \n",
       "2  international conference on management of data  1999  \n",
       "3  international conference on management of data  1999  \n",
       "4  international conference on management of data  1999  \n",
       "5  international conference on management of data  1999  \n",
       "6  international conference on management of data  1999  \n",
       "7  international conference on management of data  1999  \n",
       "8                              acm sigmod record   1998  \n",
       "9                              acm sigmod record   1998  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df_acm.columns:\n",
    "    df_acm[col].replace(r'\\s {2,}', ' ', regex=True)\n",
    "\n",
    "df_acm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8daacc69-d6f1-4a57-818b-8d744bfd1789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic integration of environmental models f...</td>\n",
       "      <td>d. scott mackay</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estimation of query-result distribution and it...</td>\n",
       "      <td>viswanath poosala, yannis e. ioannidis</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incremental maintenance for non-distributive a...</td>\n",
       "      <td>themistoklis palpanas, richard sidle, hamid pi...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cost-based selection of path expression proces...</td>\n",
       "      <td>zhao-hui tang, georges gardarin, jean-robert g...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benchmarking spatial join operations with spat...</td>\n",
       "      <td>erik g. hoel, hanan samet</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>efficient geometry-based similarity search of ...</td>\n",
       "      <td>daniel a. keim</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mining the world wide web: an information sear...</td>\n",
       "      <td>aris m. ouksel</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enhanced abstract data types in object-relatio...</td>\n",
       "      <td>praveen seshadri</td>\n",
       "      <td>vldb j.</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>report on dart '96: databases: active and real...</td>\n",
       "      <td>nandit soparkar, krithi ramamritham</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unisql's next-generation object-relational dat...</td>\n",
       "      <td>phil janus, albert d'andrea</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  semantic integration of environmental models f...   \n",
       "1  estimation of query-result distribution and it...   \n",
       "2  incremental maintenance for non-distributive a...   \n",
       "3  cost-based selection of path expression proces...   \n",
       "4  benchmarking spatial join operations with spat...   \n",
       "5  efficient geometry-based similarity search of ...   \n",
       "6  mining the world wide web: an information sear...   \n",
       "7  enhanced abstract data types in object-relatio...   \n",
       "8  report on dart '96: databases: active and real...   \n",
       "9  unisql's next-generation object-relational dat...   \n",
       "\n",
       "                                             authors              venue  year  \n",
       "0                                    d. scott mackay      sigmod record  1999  \n",
       "1             viswanath poosala, yannis e. ioannidis               vldb  1996  \n",
       "2  themistoklis palpanas, richard sidle, hamid pi...               vldb  2002  \n",
       "3  zhao-hui tang, georges gardarin, jean-robert g...               vldb  1996  \n",
       "4                          erik g. hoel, hanan samet               vldb  1995  \n",
       "5                                     daniel a. keim  sigmod conference  1999  \n",
       "6                                     aris m. ouksel      sigmod record  2002  \n",
       "7                                   praveen seshadri            vldb j.  1998  \n",
       "8                nandit soparkar, krithi ramamritham      sigmod record  1997  \n",
       "9                        phil janus, albert d'andrea      sigmod record  1996  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df_dblp2.columns:\n",
    "    df_dblp2[col].replace(r'\\s {2,}', ' ', regex=True)\n",
    "\n",
    "df_dblp2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06fc4f-c4bb-48b7-9bb9-d6262a78eed0",
   "metadata": {},
   "source": [
    "### D.\n",
    "\n",
    "Use Levenshtein similarity $(L_{sim}(S_1, S_2) = 1 - \\frac{MED(S_1, S_2}{MAX(|S_1|, |S_2|})$\n",
    "for comparing the values in the **title** attribute and compute the score $(s_t)$. (MED refers to the minimum edit distance and $|S_i|$ is the number of characters in string $S_i$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78fcfcca-8e62-4418-9ea4-4dcf275212b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest title similarity: 1.0\n",
      "ACM title: the wasa2 object-oriented workflow management system\n",
      "DBLP title: the wasa2 object-oriented workflow management system\n"
     ]
    }
   ],
   "source": [
    "# Levenshtein similarity for title comparison\n",
    "def levenshtein_similarity(s1, s2):\n",
    "    lev = Levenshtein()\n",
    "    distance = lev.get_raw_score(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "# Compare titles from ACM and DBLP datasets\n",
    "title_similarity = {}\n",
    "for i, title1 in enumerate(df_acm[\"title\"]):\n",
    "    for j, title2 in enumerate(df_dblp2[\"title\"]):\n",
    "        similarity = levenshtein_similarity(title1, title2)\n",
    "        title_similarity[(i, j)] = similarity\n",
    "\n",
    "# Find the pair with highest similarity\n",
    "max_title_sim = max(title_similarity, key=title_similarity.get)\n",
    "print(f\"Highest title similarity: {title_similarity[max_title_sim]}\")\n",
    "print(f\"ACM title: {df_acm.iloc[max_title_sim[0]]['title']}\")\n",
    "print(f\"DBLP title: {df_dblp2.iloc[max_title_sim[1]]['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafbbbd-7219-4a43-9306-89ab3d6637d4",
   "metadata": {},
   "source": [
    "### E. \n",
    "\n",
    "Use Jaro similarity to compare the values in the **authors** field and compute $(S_a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15846622-b4ca-4112-b069-86f87ce4fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest authors similarity: 1.0\n",
      "ACM author: martin bichler, arie segev, j. leon zhao\n",
      "DBLP author: martin bichler, arie segev, j. leon zhao\n"
     ]
    }
   ],
   "source": [
    "# Jaro similarity for authors comparison\n",
    "jaro = Jaro()\n",
    "\n",
    "# Compare authors from ACM and DBLP datasets\n",
    "authors_similarity = {}\n",
    "for i, author1 in enumerate(df_acm[\"authors\"]):\n",
    "    for j, author2 in enumerate(df_dblp2[\"authors\"]):\n",
    "        similarity = jaro.get_sim_score(author1, author2)\n",
    "        authors_similarity[(i, j)] = similarity\n",
    "\n",
    "# Find the pair with highest similarity\n",
    "max_authors_sim = max(authors_similarity, key=authors_similarity.get)\n",
    "print(f\"Highest authors similarity: {authors_similarity[max_authors_sim]}\")\n",
    "print(f\"ACM author: {df_acm.iloc[max_authors_sim[0]]['authors']}\")\n",
    "print(f\"DBLP author: {df_dblp2.iloc[max_authors_sim[1]]['authors']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d5145",
   "metadata": {},
   "source": [
    "### F.\n",
    "\n",
    "Use a modified version of the affine similarity that is scaled to the interval [0, 1] for the venue attribute $(S_c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae173959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest venue similarity: 1.0\n",
      "ACM venue: acm transactions on database systems (tods) \n",
      "DBLP venue: acm trans. database syst.\n"
     ]
    }
   ],
   "source": [
    "affine = Affine()\n",
    "\n",
    "venue_similarity = {}\n",
    "raw_scores = []\n",
    "\n",
    "for i, venue1 in enumerate(df_acm[\"venue\"]):\n",
    "    for j, venue2 in enumerate(df_dblp2[\"venue\"]):\n",
    "        similarity = affine.get_raw_score(str(venue1), str(venue2))\n",
    "        raw_scores.append(similarity)\n",
    "        venue_similarity[(i, j)] = similarity\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "min_score = min(raw_scores)\n",
    "max_score = max(raw_scores)\n",
    "for key in venue_similarity:\n",
    "    if max_score != min_score:\n",
    "        venue_similarity[key] = (venue_similarity[key] - min_score) / (max_score - min_score)\n",
    "    else:\n",
    "        venue_similarity[key] = 1.0  # if all scores are equal\n",
    "\n",
    "# Get max similarity\n",
    "max_venue_sim = max(venue_similarity, key=venue_similarity.get)\n",
    "print(f\"Highest venue similarity: {venue_similarity[max_venue_sim]}\")\n",
    "print(f\"ACM venue: {df_acm.iloc[max_venue_sim[0]]['venue']}\")\n",
    "print(f\"DBLP venue: {df_dblp2.iloc[max_venue_sim[1]]['venue']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56547e65",
   "metadata": {},
   "source": [
    "### G.\n",
    "\n",
    "Use Match (1) / Mismatch (0) for the year $(S_y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ebef187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact year matches: 601284 out of 6001104 comparisons\n",
      "Match rate: 0.10019556401622101\n"
     ]
    }
   ],
   "source": [
    "# Match/Mismatch for year comparison\n",
    "def year_similarity(year1, year2):\n",
    "    \"\"\"Return 1 if years match exactly, 0 otherwise\"\"\"\n",
    "    return 1.0 if year1 == year2 else 0.0\n",
    "\n",
    "# Compare years from ACM and DBLP datasets\n",
    "year_similarity_dict = {}\n",
    "for i, year1 in enumerate(df_acm[\"year\"]):\n",
    "    for j, year2 in enumerate(df_dblp2[\"year\"]):\n",
    "        similarity = year_similarity(year1, year2)\n",
    "        year_similarity_dict[(i, j)] = similarity\n",
    "\n",
    "# Count exact matches\n",
    "exact_matches = sum(1 for sim in year_similarity_dict.values() if sim == 1.0)\n",
    "total_comparisons = len(year_similarity_dict)\n",
    "print(f\"Exact year matches: {exact_matches} out of {total_comparisons} comparisons\")\n",
    "print(f\"Match rate: {exact_matches/total_comparisons}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dbe71",
   "metadata": {},
   "source": [
    "### H.\n",
    "\n",
    "Use the formula $\\text{rec\\_sim} = w_1 * s_1 + w_2 * s_2 + w_3 * s_3 + w_4 * s_4$ to combine the scores and compute the final score, where $\\sum^4_{i=1}w_i=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf308a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest combined similarity: 1.0\n",
      "ACM record: security of random data perturbation methods\n",
      "DBLP record: security of random data perturbation methods\n"
     ]
    }
   ],
   "source": [
    "# Combine scores using weighted formula: rec_sim = w1*s1 + w2*s2 + w3*s3 + w4*s4\n",
    "def combine_scores(s_t, s_a, s_c, s_y, w1=0.3, w2=0.3, w3=0.2, w4=0.2):\n",
    "    return w1 * s_t + w2 * s_a + w3 * s_c + w4 * s_y\n",
    "\n",
    "# Calculate combined similarity for all pairs\n",
    "combined_similarity = {}\n",
    "for i in range(len(df_acm)):\n",
    "    for j in range(len(df_dblp2)):\n",
    "        s_t = title_similarity.get((i, j), 0)\n",
    "        s_a = authors_similarity.get((i, j), 0)\n",
    "        s_c = venue_similarity.get((i, j), 0)\n",
    "        s_y = year_similarity_dict.get((i, j), 0)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_score = combine_scores(s_t, s_a, s_c, s_y)\n",
    "        combined_similarity[(i, j)] = combined_score\n",
    "\n",
    "# Find the pair with highest combined similarity\n",
    "max_combined = max(combined_similarity, key=combined_similarity.get)\n",
    "print(f\"Highest combined similarity: {combined_similarity[max_combined]}\")\n",
    "print(f\"ACM record: {df_acm.iloc[max_combined[0]]['title']}\")\n",
    "print(f\"DBLP record: {df_dblp2.iloc[max_combined[1]]['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1b170",
   "metadata": {},
   "source": [
    "### I.\n",
    "\n",
    "Report the records with rec_sim > 0.7 as duplicate records by storing the ids of both records in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f00ff59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2602 duplicate pairs with similarity > 0.7\n"
     ]
    }
   ],
   "source": [
    "# Report records with rec_sim > 0.7 as duplicate records\n",
    "def report_duplicates(threshold=0.7):\n",
    "    duplicates = []\n",
    "    for (i, j), score in combined_similarity.items():\n",
    "        if score > threshold:\n",
    "            duplicates.append((i, j, score))\n",
    "    \n",
    "    # Sort by similarity score (descending)\n",
    "    duplicates.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(duplicates)} duplicate pairs with similarity > {threshold}\")\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "# Report duplicates\n",
    "duplicate_pairs = report_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3795ea",
   "metadata": {},
   "source": [
    "### J.\n",
    "\n",
    "In the table `DBLP-ACM_perfectMapping.csv`, you can find the actual mappings (the ids of the correct duplicate records). Compute the precision of this method by counting the number of duplicate records that you discovered correctly. That is, among all the reported similar records by your method, how many pairs exist in the file `DBLP-ACM_perfectMapping.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92522b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8439661798616449\n",
      "True Positives: 2196\n",
      "False Positives: 406\n",
      "Total reported duplicates: 2602\n"
     ]
    }
   ],
   "source": [
    "# Reload the original datasets to get the correct IDs\n",
    "df_acm_original = pd.read_csv(\"ACM.csv\")\n",
    "df_dblp2_original = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Load the perfect mapping file\n",
    "perfect_mapping = pd.read_csv(\"DBLP-ACM_perfectMapping.csv\")\n",
    "\n",
    "perfect_pairs = set()\n",
    "for _, row in perfect_mapping.iterrows():\n",
    "    perfect_pairs.add((row['idACM'], row['idDBLP']))\n",
    "\n",
    "# Calculate precision\n",
    "def calculate_precision(reported_duplicates, perfect_pairs, df_acm_original, df_dblp2_original):\n",
    "    \"\"\"Calculate precision: TP / (TP + FP)\"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "    for acm_idx, dblp_idx, score in reported_duplicates:\n",
    "        # Get the actual IDs from the original datasets\n",
    "        acm_id = df_acm_original.iloc[acm_idx]['id']\n",
    "        dblp_id = df_dblp2_original.iloc[dblp_idx]['id']\n",
    "        \n",
    "        if (acm_id, dblp_id) in perfect_pairs:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    return precision, true_positives, false_positives\n",
    "\n",
    "# Calculate precision\n",
    "precision, tp, fp = calculate_precision(duplicate_pairs, perfect_pairs, df_acm_original, df_dblp2_original)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"Total reported duplicates: {len(duplicate_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b255e9",
   "metadata": {},
   "source": [
    "### K.\n",
    "\n",
    "Record the running time of the method. You can observe that the program takes a long time to get the results. What can you do to reduce the running time? (Just provide clear discussion â€“ no need for implementing the ideas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd2bf4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 1324.6808943748474\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Run all the code of this part to see what the total execution time is.\n",
    "\n",
    "df_acm = pd.read_csv(\"ACM.csv\")\n",
    "df_dblp2 = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\")\n",
    "df_acm.drop(\"id\", axis=1, inplace = True)\n",
    "df_dblp2.drop(\"id\", axis=1, inplace = True)\n",
    "for col in df_acm.columns:\n",
    "    df_acm[col] = df_acm[col].astype(str).str.lower()\n",
    "for col in df_dblp2.columns:\n",
    "    df_dblp2[col] = df_dblp2[col].astype(str).str.lower()\n",
    "for col in df_acm.columns:\n",
    "    df_acm[col].replace(r'\\s {2,}', ' ', regex=True)\n",
    "for col in df_dblp2.columns:\n",
    "    df_dblp2[col].replace(r'\\s {2,}', ' ', regex=True)\n",
    "\n",
    "def levenshtein_similarity(s1, s2):\n",
    "    lev = Levenshtein()\n",
    "    distance = lev.get_raw_score(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "title_similarity = {}\n",
    "for i, title1 in enumerate(df_acm[\"title\"]):\n",
    "    for j, title2 in enumerate(df_dblp2[\"title\"]):\n",
    "        similarity = levenshtein_similarity(title1, title2)\n",
    "        title_similarity[(i, j)] = similarity\n",
    "\n",
    "max_title_sim = max(title_similarity, key=title_similarity.get)\n",
    "\n",
    "jaro = Jaro()\n",
    "\n",
    "\n",
    "authors_similarity = {}\n",
    "for i, author1 in enumerate(df_acm[\"authors\"]):\n",
    "    for j, author2 in enumerate(df_dblp2[\"authors\"]):\n",
    "        similarity = jaro.get_sim_score(author1, author2)\n",
    "        authors_similarity[(i, j)] = similarity\n",
    "\n",
    "\n",
    "max_authors_sim = max(authors_similarity, key=authors_similarity.get)\n",
    "affine = Affine()\n",
    "\n",
    "venue_similarity = {}\n",
    "raw_scores = []\n",
    "\n",
    "for i, venue1 in enumerate(df_acm[\"venue\"]):\n",
    "    for j, venue2 in enumerate(df_dblp2[\"venue\"]):\n",
    "        similarity = affine.get_raw_score(str(venue1), str(venue2))\n",
    "        raw_scores.append(similarity)\n",
    "        venue_similarity[(i, j)] = similarity\n",
    "\n",
    "min_score = min(raw_scores)\n",
    "max_score = max(raw_scores)\n",
    "for key in venue_similarity:\n",
    "    if max_score != min_score:\n",
    "        venue_similarity[key] = (venue_similarity[key] - min_score) / (max_score - min_score)\n",
    "    else:\n",
    "        venue_similarity[key] = 1.0 \n",
    "\n",
    "\n",
    "max_venue_sim = max(venue_similarity, key=venue_similarity.get)\n",
    "def year_similarity(year1, year2):\n",
    "    \"\"\"Return 1 if years match exactly, 0 otherwise\"\"\"\n",
    "    return 1.0 if year1 == year2 else 0.0\n",
    "\n",
    "year_similarity_dict = {}\n",
    "for i, year1 in enumerate(df_acm[\"year\"]):\n",
    "    for j, year2 in enumerate(df_dblp2[\"year\"]):\n",
    "        similarity = year_similarity(year1, year2)\n",
    "        year_similarity_dict[(i, j)] = similarity\n",
    "\n",
    "exact_matches = sum(1 for sim in year_similarity_dict.values() if sim == 1.0)\n",
    "total_comparisons = len(year_similarity_dict)\n",
    "def combine_scores(s_t, s_a, s_c, s_y, w1=0.3, w2=0.3, w3=0.2, w4=0.2):\n",
    "    return w1 * s_t + w2 * s_a + w3 * s_c + w4 * s_y\n",
    "\n",
    "combined_similarity = {}\n",
    "for i in range(len(df_acm)):\n",
    "    for j in range(len(df_dblp2)):\n",
    "        s_t = title_similarity.get((i, j), 0)\n",
    "        s_a = authors_similarity.get((i, j), 0)\n",
    "        s_c = venue_similarity.get((i, j), 0)\n",
    "        s_y = year_similarity_dict.get((i, j), 0)\n",
    "        \n",
    "        combined_score = combine_scores(s_t, s_a, s_c, s_y)\n",
    "        combined_similarity[(i, j)] = combined_score\n",
    "\n",
    "max_combined = max(combined_similarity, key=combined_similarity.get)\n",
    "def report_duplicates(threshold=0.7):\n",
    "    duplicates = []\n",
    "    for (i, j), score in combined_similarity.items():\n",
    "        if score > threshold:\n",
    "            duplicates.append((i, j, score))\n",
    "    \n",
    "    duplicates.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "duplicate_pairs = report_duplicates()\n",
    "\n",
    "df_acm_original = pd.read_csv(\"ACM.csv\")\n",
    "df_dblp2_original = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\")\n",
    "\n",
    "perfect_mapping = pd.read_csv(\"DBLP-ACM_perfectMapping.csv\")\n",
    "\n",
    "perfect_pairs = set()\n",
    "for _, row in perfect_mapping.iterrows():\n",
    "    perfect_pairs.add((row['idACM'], row['idDBLP']))\n",
    "\n",
    "def calculate_precision(reported_duplicates, perfect_pairs, df_acm_original, df_dblp2_original):\n",
    "    \"\"\"Calculate precision: TP / (TP + FP)\"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "    for acm_idx, dblp_idx, score in reported_duplicates:\n",
    "        acm_id = df_acm_original.iloc[acm_idx]['id']\n",
    "        dblp_id = df_dblp2_original.iloc[dblp_idx]['id']\n",
    "        \n",
    "        if (acm_id, dblp_id) in perfect_pairs:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    return precision, true_positives, false_positives\n",
    "\n",
    "precision, tp, fp = calculate_precision(duplicate_pairs, perfect_pairs, df_acm_original, df_dblp2_original)\n",
    "\n",
    "\n",
    "# End of all the code.\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c529ca",
   "metadata": {},
   "source": [
    "### Total Time Taken:\n",
    "\n",
    "1324/60 = 22 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33a50b",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### step 0: Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d78b0a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journals/sigmod/Mackay99</td>\n",
       "      <td>Semantic Integration of Environmental Models f...</td>\n",
       "      <td>D. Scott Mackay</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf/vldb/PoosalaI96</td>\n",
       "      <td>Estimation of Query-Result Distribution and it...</td>\n",
       "      <td>Viswanath Poosala, Yannis E. Ioannidis</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/vldb/PalpanasSCP02</td>\n",
       "      <td>Incremental Maintenance for Non-Distributive A...</td>\n",
       "      <td>Themistoklis Palpanas, Richard Sidle, Hamid Pi...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf/vldb/GardarinGT96</td>\n",
       "      <td>Cost-based Selection of Path Expression Proces...</td>\n",
       "      <td>Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf/vldb/HoelS95</td>\n",
       "      <td>Benchmarking Spatial Join Operations with Spat...</td>\n",
       "      <td>Erik G. Hoel, Hanan Samet</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conf/sigmod/Keim99</td>\n",
       "      <td>Efficient Geometry-based Similarity Search of ...</td>\n",
       "      <td>Daniel A. Keim</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>journals/sigmod/Ouksel02</td>\n",
       "      <td>Mining the World Wide Web: An Information Sear...</td>\n",
       "      <td>Aris M. Ouksel</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>journals/vldb/Seshadri98</td>\n",
       "      <td>Enhanced Abstract Data Types in Object-Relatio...</td>\n",
       "      <td>Praveen Seshadri</td>\n",
       "      <td>VLDB J.</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>journals/sigmod/RamamrithamS97</td>\n",
       "      <td>Report on DART '96: Databases: Active and Real...</td>\n",
       "      <td>Nandit Soparkar, Krithi Ramamritham</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>journals/sigmod/DAndreaJ96</td>\n",
       "      <td>UniSQL's Next-Generation Object-Relational Dat...</td>\n",
       "      <td>Phil Janus, Albert D'Andrea</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0        journals/sigmod/Mackay99   \n",
       "1            conf/vldb/PoosalaI96   \n",
       "2         conf/vldb/PalpanasSCP02   \n",
       "3          conf/vldb/GardarinGT96   \n",
       "4               conf/vldb/HoelS95   \n",
       "5              conf/sigmod/Keim99   \n",
       "6        journals/sigmod/Ouksel02   \n",
       "7        journals/vldb/Seshadri98   \n",
       "8  journals/sigmod/RamamrithamS97   \n",
       "9      journals/sigmod/DAndreaJ96   \n",
       "\n",
       "                                               title  \\\n",
       "0  Semantic Integration of Environmental Models f...   \n",
       "1  Estimation of Query-Result Distribution and it...   \n",
       "2  Incremental Maintenance for Non-Distributive A...   \n",
       "3  Cost-based Selection of Path Expression Proces...   \n",
       "4  Benchmarking Spatial Join Operations with Spat...   \n",
       "5  Efficient Geometry-based Similarity Search of ...   \n",
       "6  Mining the World Wide Web: An Information Sear...   \n",
       "7  Enhanced Abstract Data Types in Object-Relatio...   \n",
       "8  Report on DART '96: Databases: Active and Real...   \n",
       "9  UniSQL's Next-Generation Object-Relational Dat...   \n",
       "\n",
       "                                             authors              venue  year  \n",
       "0                                    D. Scott Mackay      SIGMOD Record  1999  \n",
       "1             Viswanath Poosala, Yannis E. Ioannidis               VLDB  1996  \n",
       "2  Themistoklis Palpanas, Richard Sidle, Hamid Pi...               VLDB  2002  \n",
       "3  Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...               VLDB  1996  \n",
       "4                          Erik G. Hoel, Hanan Samet               VLDB  1995  \n",
       "5                                     Daniel A. Keim  SIGMOD Conference  1999  \n",
       "6                                     Aris M. Ouksel      SIGMOD Record  2002  \n",
       "7                                   Praveen Seshadri            VLDB J.  1998  \n",
       "8                Nandit Soparkar, Krithi Ramamritham      SIGMOD Record  1997  \n",
       "9                        Phil Janus, Albert D'Andrea      SIGMOD Record  1996  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "acm_data = pd.read_csv(\"ACM.csv\")\n",
    "dblp_data = pd.read_csv(\"DBLP2.csv\", encoding = \"latin1\")\n",
    "dblp_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014262fc",
   "metadata": {},
   "source": [
    "## Step 1, 2 & 3: Concatenate the values in each record into one single string, change all alphabetical characters into lowercase and Convert multiple spaces to one.\n",
    "\n",
    "This was done by iterating over the rows, joining all column values together as one long string, then making everything lowercase and using re.sub to turn multiple spaces into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb47ec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['journals/sigmod/mackay99 semantic integration of environmental models for application to global information systems and decision-making d. scott mackay sigmod record 1999',\n",
       " 'conf/vldb/poosalai96 estimation of query-result distribution and its application in parallel-join load balancing viswanath poosala, yannis e. ioannidis vldb 1996',\n",
       " 'conf/vldb/palpanasscp02 incremental maintenance for non-distributive aggregate functions themistoklis palpanas, richard sidle, hamid pirahesh, roberta cochrane vldb 2002']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1: concatenate values into a single string\n",
    "def concat_values(df):\n",
    "  \"\"\"\n",
    "  Takes a dataframe and returns a list of concatenated strings. Goes row by row\n",
    "  to convert all column values of that row into one long string.\n",
    "  \"\"\"\n",
    "  concat_strings = []\n",
    "  for idx, row in df.iterrows():\n",
    "    # 2: change alphabetical values into lowercase\n",
    "    string = ' '.join(row.astype(str)).lower()\n",
    "    # 3: turn multiple spaces into one use re library\n",
    "    string = re.sub(r\"\\s+\", \" \", string).strip()\n",
    "    concat_strings.append(string)\n",
    "\n",
    "  return concat_strings\n",
    "\n",
    "acm_concat = concat_values(acm_data)\n",
    "dblp_concat = concat_values(dblp_data)\n",
    "\n",
    "dblp_concat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6fa1b",
   "metadata": {},
   "source": [
    "## Step 4 & 5: Combine the records from both tables into one big list as we did during the lab, use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity.\n",
    "\n",
    "Here we combined the records from both tables with a simple '+' operation. Then we defined the functions that were used in the lab, and used them on our data. This left us with a list of sets of shingles, a vocabulary of all found shingles and a one-hot matrix that shows which shingles are in which documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecf6453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary is:13501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4910, 13501)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4: Combine the records from both tables into one big list as we did during the lab.\n",
    "combined_list = acm_concat + dblp_concat\n",
    "\n",
    "# 5: Use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity.\n",
    "def shingle(text: str, k: int)->set:\n",
    "    \"\"\"\n",
    "    Create a set of 'shingles' from the input text using k-shingling.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to be converted into shingles.\n",
    "        k (int): The length of the shingles (substring size).\n",
    "\n",
    "    Returns:\n",
    "        set: A set containing the shingles extracted from the input text.\n",
    "    \"\"\"\n",
    "    shingle_set = []\n",
    "    for i in range(len(text) - k+1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return set(shingle_set)\n",
    "\n",
    "def build_vocab(shingle_sets: list)->dict:\n",
    "    \"\"\"\n",
    "    Constructs a vocabulary dictionary from a list of shingle sets.\n",
    "\n",
    "    This function takes a list of shingle sets and creates a unified vocabulary\n",
    "    dictionary. Each unique shingle across all sets is assigned a unique integer\n",
    "    identifier.\n",
    "\n",
    "    Parameters:\n",
    "    - shingle_sets (list of set): A list containing sets of shingles.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A vocabulary dictionary where keys are the unique shingles and values\n",
    "      are their corresponding unique integer identifiers.\n",
    "\n",
    "    Example:\n",
    "    sets = [{\"apple\", \"banana\"}, {\"banana\", \"cherry\"}]\n",
    "    build_vocab(sets)\n",
    "    {'apple': 0, 'cherry': 1, 'banana': 2}  # The exact order might vary due to set behavior\n",
    "    \"\"\"\n",
    "    full_set = {item for set_ in shingle_sets for item in set_}\n",
    "    vocab = {}\n",
    "    for i, shingle in enumerate(list(full_set)):\n",
    "        vocab[shingle] = i\n",
    "    return vocab\n",
    "\n",
    "def one_hot(shingles: set, vocab: dict):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for shingle in shingles:\n",
    "        idx = vocab[shingle]\n",
    "        vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "# find shingles in the lists\n",
    "shingles = []\n",
    "k = 3\n",
    "for sentence in combined_list:\n",
    "    shingles.append(shingle(sentence,k))\n",
    "\n",
    "# build vocab using shingles\n",
    "vocab = build_vocab(shingles)\n",
    "print(f\"Number of vocabulary is:{len(vocab)}\")\n",
    "\n",
    "# create one-hot matrix for which shingles are in each document\n",
    "shingles_1hot = []\n",
    "for shingle_set in shingles:\n",
    "    shingles_1hot.append(one_hot(shingle_set,vocab))\n",
    "shingles_1hot = np.stack(shingles_1hot)\n",
    "shingles_1hot.shape # should be (no. of docs, vocab size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837cd7c",
   "metadata": {},
   "source": [
    "## Step 5 continuation: Use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity.\n",
    "\n",
    "Here we create the minhash array using the function from the lab. We used 220 hashes, because after some trial and error this gave the highest precision at the end of the pipeline.\n",
    "\n",
    "We then used the get_signature function from the lab to calculate a signature for each vector in the shingles one-hot matrix.\n",
    "\n",
    "After this, we computed the similarity between each combination of documents, and saved the ones that were >0.7 in similarity. This has complexity O(N^2), so this took around 1 minute for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b26370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dimensions of the signature matrix are (4910, 220)\n"
     ]
    }
   ],
   "source": [
    "# 5 (cont.)\n",
    "def get_minhash_arr(num_hashes:int,vocab:dict):\n",
    "    \"\"\"\n",
    "    Generates a MinHash array for the given vocabulary.\n",
    "\n",
    "    This function creates an array where each row represents a hash function and\n",
    "    each column corresponds to a word in the vocabulary. The values are permutations\n",
    "    of integers representing the hashed value of each word for that particular hash function.\n",
    "\n",
    "    Parameters:\n",
    "    - num_hashes (int): The number of hash functions (rows) to generate for the MinHash array.\n",
    "    - vocab (dict): The vocabulary where keys are words and values can be any data\n",
    "      (only keys are used in this function).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The generated MinHash array with `num_hashes` rows and columns equal\n",
    "      to the size of the vocabulary. Each cell contains the hashed value of the corresponding\n",
    "      word for the respective hash function.\n",
    "\n",
    "    Example:\n",
    "    vocab = {'apple': 1, 'banana': 2}\n",
    "    get_minhash_arr(2, vocab)\n",
    "    # Possible output:\n",
    "    # array([[1, 2],\n",
    "    #        [2, 1]])\n",
    "    \"\"\"\n",
    "    length = len(vocab.keys())\n",
    "    arr = np.zeros((num_hashes,length))\n",
    "    for i in range(num_hashes):\n",
    "        permutation = np.random.permutation(len(vocab.keys())) + 1\n",
    "        arr[i,:] = permutation.copy()\n",
    "    return arr.astype(int)\n",
    "\n",
    "def get_signature(minhash:np.ndarray, vector:np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the signature of a given vector using the provided MinHash matrix.\n",
    "\n",
    "    The function finds the nonzero indices of the vector, extracts the corresponding\n",
    "    columns from the MinHash matrix, and computes the signature as the minimum value\n",
    "    across those columns for each row of the MinHash matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - minhash (np.ndarray): The MinHash matrix where each column represents a shingle\n",
    "      and each row represents a hash function.\n",
    "    - vector (np.ndarray): A vector representing the presence (non-zero values) or\n",
    "      absence (zero values) of shingles.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The signature vector derived from the MinHash matrix for the provided vector.\n",
    "\n",
    "    Example:\n",
    "    minhash = np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]])\n",
    "    vector = np.array([0, 1, 0])\n",
    "    get_signature(minhash, vector)\n",
    "    output:array([3, 6, 9])\n",
    "    \"\"\"\n",
    "    idx = np.nonzero(vector)[0].tolist()\n",
    "    shingles = minhash[:,idx]\n",
    "    signature = np.min(shingles,axis=1)\n",
    "    return signature\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    return intersection_size / union_size if union_size != 0 else 0.0\n",
    "\n",
    "def compute_signature_similarity(signature_1, signature_2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two signature matrices using MinHash.\n",
    "\n",
    "    Parameters:\n",
    "    - signature_1: First signature matrix as a numpy array.\n",
    "    - signature_matrix2: Second signature matrix as a numpy array.\n",
    "\n",
    "    Returns:\n",
    "    - Estimated Jaccard similarity.\n",
    "    \"\"\"\n",
    "    # Ensure the matrices have the same shape\n",
    "    if signature_1.shape != signature_2.shape:\n",
    "        raise ValueError(\"Both signature matrices must have the same shape.\")\n",
    "    # Count the number of rows where the two matrices agree\n",
    "    agreement_count = np.sum(signature_1 == signature_2)\n",
    "    # Calculate the similarity\n",
    "    similarity = agreement_count / signature_2.shape[0]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# array to hold 220 permutations\n",
    "minhash_arr =  get_minhash_arr(220,vocab)\n",
    "signatures = []\n",
    "\n",
    "# create signature matrix\n",
    "for vector in shingles_1hot:\n",
    "    signatures.append(get_signature(minhash_arr,vector))\n",
    "signatures = np.stack(signatures)\n",
    "print(\"the dimensions of the signature matrix are\", signatures.shape)\n",
    "\n",
    "# compute similarity\n",
    "high_sim = []\n",
    "for i in range(len(signatures)):\n",
    "  for j in range(i + 1, len(signatures)):\n",
    "    # if combination has high similarity in both measures we save it\n",
    "    if compute_signature_similarity(signatures[i], signatures[j]) > 0.7 and jaccard_similarity(shingles[i], shingles[j]) > 0.7:\n",
    "      high_sim.append((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d36b4d",
   "metadata": {},
   "source": [
    "## Step 6: Extract the top 2224 candidates from the LSH algorithm, compare them to the actual mappings in the file DBLP-ACM_perfectMapping.csv and compute the precision of the method.\n",
    "\n",
    "Here we once again used code from the lab to create the LSH class. We simply run the required functions to get a mapping, and taking the top 2224 will give us the 2224 combinations that are most likely to be the same documents.\n",
    "\n",
    "20 buckets is once again because we found this to be a good number after a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ca8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Extract the top 2224 candidates from the LSH algorithm, compare them to\n",
    "# the actual mappings in the file DBLP-ACM_perfectMapping.csv and compute the\n",
    "# precision of the method.\n",
    "class LSH:\n",
    "    \"\"\"\n",
    "    Implements the Locality Sensitive Hashing (LSH) technique for approximate\n",
    "    nearest neighbor search.\n",
    "    \"\"\"\n",
    "    buckets = []\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, b: int):\n",
    "        \"\"\"\n",
    "        Initializes the LSH instance with a specified number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - b (int): The number of bands to divide the signature into.\n",
    "        \"\"\"\n",
    "        self.b = b\n",
    "        for i in range(b):\n",
    "            self.buckets.append({})\n",
    "\n",
    "    def make_subvecs(self, signature: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Divides a given signature into subvectors based on the number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be divided.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: A stacked array where each row is a subvector of the signature.\n",
    "        \"\"\"\n",
    "        l = len(signature)\n",
    "        assert l % self.b == 0\n",
    "        r = int(l / self.b)\n",
    "        subvecs = []\n",
    "        for i in range(0, l, r):\n",
    "            subvecs.append(signature[i:i+r])\n",
    "        return np.stack(subvecs)\n",
    "\n",
    "    def add_hash(self, signature: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds a signature to the appropriate LSH buckets based on its subvectors.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be hashed and added.\n",
    "        \"\"\"\n",
    "        subvecs = self.make_subvecs(signature).astype(str)\n",
    "        for i, subvec in enumerate(subvecs):\n",
    "            subvec = ','.join(subvec)\n",
    "            if subvec not in self.buckets[i].keys():\n",
    "                self.buckets[i][subvec] = []\n",
    "            self.buckets[i][subvec].append(self.counter)\n",
    "        self.counter += 1\n",
    "\n",
    "    def check_candidates(self) -> set:\n",
    "        \"\"\"\n",
    "        Identifies candidate pairs from the LSH buckets that could be potential near duplicates.\n",
    "\n",
    "        Returns:\n",
    "        - set: A set of tuple pairs representing the indices of candidate signatures.\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        for bucket_band in self.buckets:\n",
    "            keys = bucket_band.keys()\n",
    "            for bucket in keys:\n",
    "                hits = bucket_band[bucket]\n",
    "                if len(hits) > 1:\n",
    "                    candidates.extend(combinations(hits, 2))\n",
    "        return set(candidates)\n",
    "\n",
    "# set 20 buckets and create LSH object\n",
    "b = 20\n",
    "lsh = LSH(b)\n",
    "for signature in signatures:\n",
    "    lsh.add_hash(signature)\n",
    "candidate_pairs = lsh.check_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6df97",
   "metadata": {},
   "source": [
    "## Step 6 Continuation\n",
    "Now we read in the perfect mapping data and convert the data into a format that is the same as the data we have saved already.\n",
    "We check every mapping against the top 2224 mappings that we had found, and save the combinations that we found that were correct. We then calculate the precision of finding documents that are the same using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff81975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of LSH method: 0.5516093229744728\n"
     ]
    }
   ],
   "source": [
    "# 6 (cont.)\n",
    "perfect_map = pd.read_csv(\"DBLP-ACM_perfectMapping.csv\")\n",
    "perfect_map\n",
    "\n",
    "# convert the dataframe to set of tuples, so it is the same format as top_candidates\n",
    "perfect_mapping = set(zip(perfect_map['idDBLP'], perfect_map['idACM']))\n",
    "\n",
    "# get the top 2224 candidates indices and then actual ids\n",
    "top_pairs = list(candidate_pairs)[:2224]\n",
    "\n",
    "# create lists of ids and put them together\n",
    "acm_ids = list(acm_data['id'])\n",
    "dblp_ids = list(dblp_data['id'])\n",
    "combined_ids = acm_ids + dblp_ids\n",
    "\n",
    "# create a list of sets of ids so that we can compare to the perfect mapping\n",
    "top_pairs_ids = []\n",
    "for idx1, idx2 in top_pairs:\n",
    "    id1 = combined_ids[idx1]\n",
    "    id2 = combined_ids[idx2]\n",
    "    top_pairs_ids.append((id1, id2))\n",
    "\n",
    "# find out how many pairs are mapped correctly\n",
    "correct_count = 0\n",
    "for pair in top_pairs_ids:\n",
    "    # check every top pair against the pairs in perfect mapping\n",
    "    if pair in perfect_mapping or (pair[1], pair[0]) in perfect_mapping:\n",
    "        correct_count += 1\n",
    "\n",
    "# precision is amount correct / total candidates\n",
    "precision = correct_count / len(top_pairs)\n",
    "print(\"Precision of LSH method:\", precision)\n",
    "\n",
    "# runtime of whole notebook was 8 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25552f2f",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Extract the top 2224 candidates from the LSH algorithm, compare them to the actual mappings in the file `DBLP-ACM_perfectMapping.csv` and compute the precision of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "850ec9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    \"\"\"\n",
    "    Implements the Locality Sensitive Hashing (LSH) technique for approximate\n",
    "    nearest neighbor search.\n",
    "    \"\"\"\n",
    "    buckets = []\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, b: int):\n",
    "        \"\"\"\n",
    "        Initializes the LSH instance with a specified number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - b (int): The number of bands to divide the signature into.\n",
    "        \"\"\"\n",
    "        self.b = b\n",
    "        for i in range(b):\n",
    "            self.buckets.append({})\n",
    "\n",
    "    def make_subvecs(self, signature: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Divides a given signature into subvectors based on the number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be divided.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: A stacked array where each row is a subvector of the signature.\n",
    "        \"\"\"\n",
    "        l = len(signature)\n",
    "        assert l % self.b == 0\n",
    "        r = int(l / self.b)\n",
    "        subvecs = []\n",
    "        for i in range(0, l, r):\n",
    "            subvecs.append(signature[i:i+r])\n",
    "        return np.stack(subvecs)\n",
    "\n",
    "    def add_hash(self, signature: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds a signature to the appropriate LSH buckets based on its subvectors.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be hashed and added.\n",
    "        \"\"\"\n",
    "        subvecs = self.make_subvecs(signature).astype(str)\n",
    "        for i, subvec in enumerate(subvecs):\n",
    "            subvec = ','.join(subvec)\n",
    "            if subvec not in self.buckets[i].keys():\n",
    "                self.buckets[i][subvec] = []\n",
    "            self.buckets[i][subvec].append(self.counter)\n",
    "        self.counter += 1\n",
    "\n",
    "    def check_candidates(self) -> set:\n",
    "        \"\"\"\n",
    "        Identifies candidate pairs from the LSH buckets that could be potential near duplicates.\n",
    "\n",
    "        Returns:\n",
    "        - set: A set of tuple pairs representing the indices of candidate signatures.\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        for bucket_band in self.buckets:\n",
    "            keys = bucket_band.keys()\n",
    "            for bucket in keys:\n",
    "                hits = bucket_band[bucket]\n",
    "                if len(hits) > 1:\n",
    "                    candidates.extend(combinations(hits, 2))\n",
    "        return set(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71ede169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of LSH method: 0.560488346281909\n"
     ]
    }
   ],
   "source": [
    "top_pairs_ids = []\n",
    "for idx1, idx2 in high_sim:\n",
    "    id1 = combined_ids[idx1]\n",
    "    id2 = combined_ids[idx2]\n",
    "    top_pairs_ids.append((id1, id2))\n",
    "\n",
    "# find out how many pairs are mapped correctly\n",
    "correct_count = 0\n",
    "for pair in top_pairs_ids:\n",
    "    # check every top pair against the pairs in perfect mapping\n",
    "    if pair in perfect_mapping or (pair[1], pair[0]) in perfect_mapping:\n",
    "        correct_count += 1\n",
    "\n",
    "# precision is amount correct / total candidates\n",
    "precision = correct_count / len(top_pairs)\n",
    "print(\"Precision of LSH method:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2958a",
   "metadata": {},
   "source": [
    "### 7. Record the running time of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d949bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 49.95915746688843\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "acm_data = pd.read_csv(\"ACM.csv\")\n",
    "dblp_data = pd.read_csv(\"DBLP2.csv\", encoding = \"latin1\")\n",
    "\n",
    "acm_concat = concat_values(acm_data)\n",
    "dblp_concat = concat_values(dblp_data)\n",
    "\n",
    "shingles = []\n",
    "k = 3\n",
    "for sentence in combined_list:\n",
    "    shingles.append(shingle(sentence,k))\n",
    "\n",
    "vocab = build_vocab(shingles)\n",
    "\n",
    "shingles_1hot = []\n",
    "for shingle_set in shingles:\n",
    "    shingles_1hot.append(one_hot(shingle_set,vocab))\n",
    "shingles_1hot = np.stack(shingles_1hot)\n",
    "shingles_1hot.shape \n",
    "\n",
    "minhash_arr =  get_minhash_arr(220,vocab)\n",
    "signatures = []\n",
    "\n",
    "for vector in shingles_1hot:\n",
    "    signatures.append(get_signature(minhash_arr,vector))\n",
    "signatures = np.stack(signatures)\n",
    "\n",
    "high_sim = []\n",
    "for i in range(len(signatures)):\n",
    "  for j in range(i + 1, len(signatures)):\n",
    "    if compute_signature_similarity(signatures[i], signatures[j]) > 0.7 and jaccard_similarity(shingles[i], shingles[j]) > 0.7:\n",
    "      high_sim.append((i, j))\n",
    "\n",
    "b = 20\n",
    "lsh = LSH(b)\n",
    "for signature in signatures:\n",
    "    lsh.add_hash(signature)\n",
    "candidate_pairs = lsh.check_candidates()\n",
    "\n",
    "perfect_map = pd.read_csv(\"DBLP-ACM_perfectMapping.csv\")\n",
    "perfect_map\n",
    "\n",
    "perfect_mapping = set(zip(perfect_map['idDBLP'], perfect_map['idACM']))\n",
    "\n",
    "top_pairs = list(candidate_pairs)[:2224]\n",
    "\n",
    "acm_ids = list(acm_data['id'])\n",
    "dblp_ids = list(dblp_data['id'])\n",
    "combined_ids = acm_ids + dblp_ids\n",
    "\n",
    "top_pairs_ids = []\n",
    "for idx1, idx2 in top_pairs:\n",
    "    id1 = combined_ids[idx1]\n",
    "    id2 = combined_ids[idx2]\n",
    "    top_pairs_ids.append((id1, id2))\n",
    "\n",
    "correct_count = 0\n",
    "for pair in top_pairs_ids:\n",
    "    if pair in perfect_mapping or (pair[1], pair[0]) in perfect_mapping:\n",
    "        correct_count += 1\n",
    "\n",
    "precision = correct_count / len(top_pairs)\n",
    "\n",
    "\n",
    "top_pairs_ids = []\n",
    "for idx1, idx2 in high_sim:\n",
    "    id1 = combined_ids[idx1]\n",
    "    id2 = combined_ids[idx2]\n",
    "    top_pairs_ids.append((id1, id2))\n",
    "\n",
    "correct_count = 0\n",
    "for pair in top_pairs_ids:\n",
    "    if pair in perfect_mapping or (pair[1], pair[0]) in perfect_mapping:\n",
    "        correct_count += 1\n",
    "\n",
    "precision = correct_count / len(top_pairs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8998d",
   "metadata": {},
   "source": [
    "## Task 3. Data preperation\n",
    "\n",
    "For this task, use the Pima Indians Diabetes Database.\n",
    "\n",
    "### 1. \n",
    "\n",
    "Compute the correlation between the different columns after removing the `outcome` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36e5939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading csv file\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e691701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.141282</td>\n",
       "      <td>-0.081672</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.239528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>-0.081672</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>-0.113970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.036242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
       "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
       "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
       "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
       "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
       "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
       "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
       "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
       "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
       "\n",
       "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
       "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
       "Glucose                   0.331357  0.221071                  0.137337   \n",
       "BloodPressure             0.088933  0.281805                  0.041265   \n",
       "SkinThickness             0.436783  0.392573                  0.183928   \n",
       "Insulin                   1.000000  0.197859                  0.185071   \n",
       "BMI                       0.197859  1.000000                  0.140647   \n",
       "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
       "Age                      -0.042163  0.036242                  0.033561   \n",
       "\n",
       "                               Age  \n",
       "Pregnancies               0.544341  \n",
       "Glucose                   0.263514  \n",
       "BloodPressure             0.239528  \n",
       "SkinThickness            -0.113970  \n",
       "Insulin                  -0.042163  \n",
       "BMI                       0.036242  \n",
       "DiabetesPedigreeFunction  0.033561  \n",
       "Age                       1.000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating correlation before cleaning\n",
    "cor_raw = df.drop(columns=[\"Outcome\"]).corr()\n",
    "cor_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64c591",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Remove the disguised values from the table. We need to remove the values that equal to `0` from columns `BloodPressure`, `SkinThickness` and `BMI` as these are missing values but they have been replaced by the value `0`. Remove the value but keep the record (i.e.) change the value to `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc56d14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148           72.0           35.0        0  33.6   \n",
       "1            1       85           66.0           29.0        0  26.6   \n",
       "2            8      183           64.0            NaN        0  23.3   \n",
       "3            1       89           66.0           23.0       94  28.1   \n",
       "4            0      137           40.0           35.0      168  43.1   \n",
       "5            5      116           74.0            NaN        0  25.6   \n",
       "6            3       78           50.0           32.0       88  31.0   \n",
       "7           10      115            NaN            NaN        0  35.3   \n",
       "8            2      197           70.0           45.0      543  30.5   \n",
       "9            8      125           96.0            NaN        0   NaN   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  \n",
       "5                     0.201   30        0  \n",
       "6                     0.248   26        1  \n",
       "7                     0.134   29        0  \n",
       "8                     0.158   53        1  \n",
       "9                     0.232   54        1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replacing 0 values with NaN\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned[[\"BloodPressure\", \"SkinThickness\", \"BMI\"]] = df_cleaned[[\"BloodPressure\", \"SkinThickness\", \"BMI\"]].replace(0, np.nan)\n",
    "df_cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce5368",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Fill the cells with `null` using the mean values of the records that have the same class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9aa65c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>94</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>168</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>27.235457</td>\n",
       "      <td>0</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>70.877339</td>\n",
       "      <td>27.235457</td>\n",
       "      <td>0</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>543</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>35.406767</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin        BMI  \\\n",
       "0            6      148      72.000000      35.000000        0  33.600000   \n",
       "1            1       85      66.000000      29.000000        0  26.600000   \n",
       "2            8      183      64.000000      33.000000        0  23.300000   \n",
       "3            1       89      66.000000      23.000000       94  28.100000   \n",
       "4            0      137      40.000000      35.000000      168  43.100000   \n",
       "5            5      116      74.000000      27.235457        0  25.600000   \n",
       "6            3       78      50.000000      32.000000       88  31.000000   \n",
       "7           10      115      70.877339      27.235457        0  35.300000   \n",
       "8            2      197      70.000000      45.000000      543  30.500000   \n",
       "9            8      125      96.000000      33.000000        0  35.406767   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  \n",
       "5                     0.201   30        0  \n",
       "6                     0.248   26        1  \n",
       "7                     0.134   29        0  \n",
       "8                     0.158   53        1  \n",
       "9                     0.232   54        1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filling NaN values with the mean per group based on Outcome\n",
    "for col in [\"BloodPressure\", \"SkinThickness\", \"BMI\"]:\n",
    "    df_cleaned[col] = df_cleaned.groupby(\"Outcome\")[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df_cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db4494",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Compute the correlation between the different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2a2133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.208935</td>\n",
       "      <td>0.094172</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222417</td>\n",
       "      <td>0.220943</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.219879</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.208935</td>\n",
       "      <td>0.222417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>-0.048106</td>\n",
       "      <td>0.286518</td>\n",
       "      <td>-0.002264</td>\n",
       "      <td>0.324439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>0.094172</td>\n",
       "      <td>0.220943</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104017</td>\n",
       "      <td>0.565443</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>0.135916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>-0.048106</td>\n",
       "      <td>0.104017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185545</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.219879</td>\n",
       "      <td>0.286518</td>\n",
       "      <td>0.565443</td>\n",
       "      <td>0.185545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152530</td>\n",
       "      <td>0.027578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>-0.002264</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.152530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.324439</td>\n",
       "      <td>0.135916</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
       "Pregnancies                  1.000000  0.129459       0.208935       0.094172   \n",
       "Glucose                      0.129459  1.000000       0.222417       0.220943   \n",
       "BloodPressure                0.208935  0.222417       1.000000       0.203453   \n",
       "SkinThickness                0.094172  0.220943       0.203453       1.000000   \n",
       "Insulin                     -0.073535  0.331357      -0.048106       0.104017   \n",
       "BMI                          0.024127  0.219879       0.286518       0.565443   \n",
       "DiabetesPedigreeFunction    -0.033523  0.137337      -0.002264       0.102426   \n",
       "Age                          0.544341  0.263514       0.324439       0.135916   \n",
       "\n",
       "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
       "Pregnancies              -0.073535  0.024127                 -0.033523   \n",
       "Glucose                   0.331357  0.219879                  0.137337   \n",
       "BloodPressure            -0.048106  0.286518                 -0.002264   \n",
       "SkinThickness             0.104017  0.565443                  0.102426   \n",
       "Insulin                   1.000000  0.185545                  0.185071   \n",
       "BMI                       0.185545  1.000000                  0.152530   \n",
       "DiabetesPedigreeFunction  0.185071  0.152530                  1.000000   \n",
       "Age                      -0.042163  0.027578                  0.033561   \n",
       "\n",
       "                               Age  \n",
       "Pregnancies               0.544341  \n",
       "Glucose                   0.263514  \n",
       "BloodPressure             0.324439  \n",
       "SkinThickness             0.135916  \n",
       "Insulin                  -0.042163  \n",
       "BMI                       0.027578  \n",
       "DiabetesPedigreeFunction  0.033561  \n",
       "Age                       1.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the correlation of the cleaned data\n",
    "cor_clean = df_cleaned.drop(columns=[\"Outcome\"]).corr()\n",
    "cor_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34351176",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Compare the values from this step with the values in the first step (just mention the most important changes (if any)) and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e2e9b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067653</td>\n",
       "      <td>0.175844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069828</td>\n",
       "      <td>0.163615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.067653</td>\n",
       "      <td>0.069828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003918</td>\n",
       "      <td>-0.137039</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>-0.043529</td>\n",
       "      <td>0.084911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>0.175844</td>\n",
       "      <td>0.163615</td>\n",
       "      <td>-0.003918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.332766</td>\n",
       "      <td>0.172869</td>\n",
       "      <td>-0.081501</td>\n",
       "      <td>0.249886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.137039</td>\n",
       "      <td>-0.332766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.006444</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.172869</td>\n",
       "      <td>-0.012314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>-0.008664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043529</td>\n",
       "      <td>-0.081501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084911</td>\n",
       "      <td>0.249886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
       "Pregnancies                  0.000000  0.000000       0.067653       0.175844   \n",
       "Glucose                      0.000000  0.000000       0.069828       0.163615   \n",
       "BloodPressure                0.067653  0.069828       0.000000      -0.003918   \n",
       "SkinThickness                0.175844  0.163615      -0.003918       0.000000   \n",
       "Insulin                      0.000000  0.000000      -0.137039      -0.332766   \n",
       "BMI                          0.006444 -0.001192       0.004713       0.172869   \n",
       "DiabetesPedigreeFunction     0.000000  0.000000      -0.043529      -0.081501   \n",
       "Age                          0.000000  0.000000       0.084911       0.249886   \n",
       "\n",
       "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
       "Pregnancies               0.000000  0.006444                  0.000000   \n",
       "Glucose                   0.000000 -0.001192                  0.000000   \n",
       "BloodPressure            -0.137039  0.004713                 -0.043529   \n",
       "SkinThickness            -0.332766  0.172869                 -0.081501   \n",
       "Insulin                   0.000000 -0.012314                  0.000000   \n",
       "BMI                      -0.012314  0.000000                  0.011884   \n",
       "DiabetesPedigreeFunction  0.000000  0.011884                  0.000000   \n",
       "Age                       0.000000 -0.008664                  0.000000   \n",
       "\n",
       "                               Age  \n",
       "Pregnancies               0.000000  \n",
       "Glucose                   0.000000  \n",
       "BloodPressure             0.084911  \n",
       "SkinThickness             0.249886  \n",
       "Insulin                   0.000000  \n",
       "BMI                      -0.008664  \n",
       "DiabetesPedigreeFunction  0.000000  \n",
       "Age                       0.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating difference between clean and raw correlation\n",
    "cor_diff = cor_clean - cor_raw\n",
    "cor_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
